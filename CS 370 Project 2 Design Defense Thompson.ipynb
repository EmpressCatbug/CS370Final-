{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Defense \n",
    "\n",
    "  \n",
    "\n",
    "#### 1. Examine the Distinctions Between Human and Machine Methods of Problem Solving \n",
    "\n",
    "  \n",
    "\n",
    "​​**Human Approach to Solving the Maze​**: \n",
    "\n",
    "- **Observation**: ​A human​ would start by observing the maze to understand its layout, identifying obstacles and possible paths. \n",
    "\n",
    "- **Planning**: The human would plan a route mentally, considering the shortest or safest paths to the treasure while avoiding obstacles. \n",
    "\n",
    "- **Trial and Error**: They might use a trial-and-error approach, trying different paths and backtracking if they encounter dead ends. \n",
    "\n",
    "- **Learning and Memory**: If attempting the maze multiple times, the human would learn from previous attempts, remembering successful paths and avoiding past mistakes. \n",
    "\n",
    "- **Goal-Oriented Actions**: The human remains focused on reaching the treasure, adjusting their strategy based on new observations. \n",
    "\n",
    "  \n",
    "\n",
    "**​Intelligent Agent's Approach**:​ \n",
    "\n",
    "​​- **Initialization**: The agent initializes its Q-table or​ neural network weights and begins without any past knowledge ​of the maze.​ \n",
    "\n",
    "- **Exploration**: Initially, the agent explores the environment randomly to gather information about the states and rewards associated with different actions. For example, the agent uses `epsilon-greedy` policy to explore the maze: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  def choose_action(state, epsilon): \n",
    "\n",
    "      if np.random.rand() < epsilon: \n",
    "\n",
    "          ​return np.random.randint​(0, 4) \n",
    "\n",
    "      else: \n",
    "\n",
    "          ​q_values = model.predict​(state) \n",
    "\n",
    "          return ​np.argmax(q_values[0​]) \n",
    "\n",
    "  ``` \n",
    "\n",
    "- **​Q-Learning Algorithm**: Based on the​ incentives it receives, the agent updates its knowledge using ​the Q-learning algorithm​. It ​uses the Bellman equation to update​ the Q-values continuously: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  def q_learning_train(maze, agent, episodes, gamma, epsilon): \n",
    "\n",
    "      ​for episode in range​(episodes): \n",
    "\n",
    "          state = maze.​reset()​ \n",
    "\n",
    "          ​done = False​ \n",
    "\n",
    "          ​while not done:​ \n",
    "\n",
    "              ​action = choose_action(state, epsilon)​ \n",
    "\n",
    "              ​next_state, reward, done​ = maze.​step(action​) \n",
    "\n",
    "              target = reward + gamma * np.max(agent.predict(next_state)) \n",
    "\n",
    "              q_values = agent.predict(state) \n",
    "\n",
    "              q_values[0][action] = target \n",
    "\n",
    "              agent.fit(state, q_values, epochs=1, verbose=0) \n",
    "\n",
    "              ​state = next_state​ \n",
    "\n",
    "  ``` \n",
    "\n",
    "- **Exploitation and Exploration**: The agent strikes a balance between using well-traveled routes that provide large rewards (exploitation) and discovering new paths (exploration).  \n",
    "\n",
    "- **Training and Convergence**: The agent's policy eventually converges to an ideal course of action through a series of events, decreasing the distance to the treasure while dodging hazards. \n",
    "\n",
    "  \n",
    "\n",
    "**​Similarities and Differences​**: \n",
    "\n",
    "- **Similarities**: \n",
    "\n",
    "  - ​Both humans and​ the agent ​use trial and error​ to find the optimal path. \n",
    "\n",
    "  - Both approaches involve learning from past experiences to improve future decisions. \n",
    "\n",
    "  - Both aim ​to reach the goal​ (treasure) while avoiding obstacles. \n",
    "\n",
    "  \n",
    "\n",
    "- **Differences**: \n",
    "\n",
    "  ​- Humans rely on cognitive​ abilities and memory, ​whereas the agent​ relies on algorithms and computational processes. \n",
    "\n",
    "  - Humans can plan and visualize potential routes beforehand, while the agent iteratively improves its strategy through reinforcement learning. \n",
    "\n",
    "  - The agent requires a defined reward structure to learn effectively, whereas humans can adapt to various goals and rules intuitively. \n",
    "\n",
    "  \n",
    "\n",
    "#### 2. ​Assess the Purpose of the Intelligent Agent in Pathfinding​ \n",
    "\n",
    "  \n",
    "\n",
    "**Exploitation vs. Exploration**: \n",
    "\n",
    "- **Exploitation**: Use the previously acquired knowledge to execute the most well-known action. Based on prior results, this focuses on optimizing the immediate payoff. \n",
    "\n",
    "- **Exploration**: Trying out new actions to discover their potential rewards. This is crucial for learning and improving the policy. \n",
    "\n",
    "- **Ideal Proportion**: The ideal proportion of exploitation and exploration is often achieved through a decaying epsilon strategy in epsilon-greedy algorithms. ​Initially, a​ ​higher exploration rate (e.g., epsilon = 1​) is used to gather information, which gradually decreases to a lower value (e.g., epsilon = 0.1) to favor exploitation as the agent becomes more confident in its knowledge. This equilibrium guarantees that the agent sufficiently investigates to gain an understanding of the surroundings, all the while utilizing existing knowledge to maximize efficiency: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  epsilon = max(epsilon_min, epsilon_decay * epsilon) \n",
    "\n",
    "  ``` \n",
    "\n",
    "  \n",
    "\n",
    "**Reinforcement Learning for Pathfinding**: \n",
    "\n",
    "- **Policy Learning**: By linking states with the most effective actions, reinforcement learning assists the agent in learning the optimum course of action to traverse the maze. \n",
    "\n",
    "- **Reward Signals**: When the agent overcomes hurdles and reaches the treasure, they are rewarded. The learning process is guided by these reward signals: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  reward = -1 \n",
    "\n",
    "  if state == treasure_state: \n",
    "\n",
    "      reward = 100 \n",
    "\n",
    "  ``` \n",
    "\n",
    "- **Q-Learning Updates**: ​Based on the Bellman equation, which​ considers both the reward that was received ​and the maximum​ reward that is anticipated in the future, the agent adjusts its Q-values. This aids in the agent's steady learning of the worth of various states and activities: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  target = reward + gamma * np.max(agent.predict(next_state)) \n",
    "\n",
    "  ``` \n",
    "\n",
    "  \n",
    "\n",
    "#### 3. ​Evaluate the Use of Algorithms to Solve Complex Problems​ \n",
    "\n",
    "  \n",
    "\n",
    "​​**Implementation of Deep Q-Learning​**: \n",
    "\n",
    "- **Neural Network Structure**: ​The Q-value function​ is approximated using ​a neural​ ​network. An input layer, hidden layers, and an output layer​ that corresponds to the action space make up the network: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  ​model = Sequential()​ \n",
    "\n",
    "  ​model.add(Dense(24, input_dim=state_size, activation='relu'))​ \n",
    "\n",
    "  ​model.add(Dense(24, activation='relu'))​ \n",
    "\n",
    "  ​model.add(Dense(action_size, activation='linear'))​ \n",
    "\n",
    "  ​model.compile(loss='mse', optimizer=Adam(lr=learning_rate​)) \n",
    "\n",
    "  ``` \n",
    "\n",
    "- **State Representation**: The neural network is fed with ​the current state of the​ maze, which includes the agent's position as well as the locations of the obstacles and treasure. \n",
    "\n",
    "- **Action Selection**: For choosing an action and weighing exploration versus exploitation, ​an epsilon-greedy policy​ is employed: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  if np.random.rand() < epsilon: \n",
    "\n",
    "      ​action = np.random.randint(0, action_size)​ \n",
    "\n",
    "  else: \n",
    "\n",
    "      ​action = np.argmax(model.predict(state)[0​]) \n",
    "\n",
    "  ``` \n",
    "\n",
    "- **Experience Replay**: In a replay buffer, the agent records its experiences (state, ​action, reward, and next state). The​ Q-network is updated during training using random samples from this buffer, which increases learning efficiency and stability: \n",
    "\n",
    "  ```python \n",
    "\n",
    "  replay_buffer.​append((state, action, reward, next_state, done​)) \n",
    "\n",
    "  if len(replay_buffer) > ​batch_size:​ \n",
    "\n",
    "      ​minibatch = random.sample​(replay_buffer, ​batch_size)​ \n",
    "\n",
    "      ​for state, action, reward, next_state, done in minibatch:​ \n",
    "\n",
    "          target = reward \n",
    "\n",
    "          ​if not done:​ \n",
    "\n",
    "              ​target = reward + gamma * np.amax(model.predict(next_state)[0])​ \n",
    "\n",
    "          ​target_f = model.predict(state)​ \n",
    "\n",
    "          ​target_f[0][action] = target​ \n",
    "\n",
    "          ​model.fit(state, target_f, epochs=1, verbose=0​) \n",
    "\n",
    "  ``` \n",
    "\n",
    "- **Target Network**: Divergence in the learning process is minimized by computing ​the target​ Q-values using a different target network. \n",
    "\n",
    "- **Training Loop**: The agent iterates through episodes, collecting experiences, updating the Q-network, and gradually improving its policy ​to find the optimal path to​ the treasure. \n",
    "\n",
    "  \n",
    "\n",
    "### ​References​  \n",
    "\n",
    "​​- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd​ ​ed.). MIT Press.​ \n",
    "\n",
    "​​- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... &​ ​Hassabis, D. (2015). Human-level control through deep reinforcement learning.​ ​*Nature*, 518(7540), 529-533​. \n",
    "\n",
    "- ​Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press​. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
